{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfilelist=[]\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        filelist.append(os.path.join(dirname, filename))\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-27T11:11:48.851711Z","iopub.execute_input":"2021-11-27T11:11:48.852152Z","iopub.status.idle":"2021-11-27T11:11:48.872466Z","shell.execute_reply.started":"2021-11-27T11:11:48.852108Z","shell.execute_reply":"2021-11-27T11:11:48.871788Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# install the required libraries\n!pip install SpeechRecognition\n!pip install contractions","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:11:49.510078Z","iopub.execute_input":"2021-11-27T11:11:49.510906Z","iopub.status.idle":"2021-11-27T11:12:11.717737Z","shell.execute_reply.started":"2021-11-27T11:11:49.510861Z","shell.execute_reply":"2021-11-27T11:12:11.716992Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# import the libraries \nimport IPython.display as ipd\nimport speech_recognition as sr\nimport textblob\nimport random\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim import corpora, models\n\nimport gensim\nimport nltk\nimport contractions\n\nimport spacy\nfrom spacy import displacy\n\n# libraries for visualization\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:12:11.719721Z","iopub.execute_input":"2021-11-27T11:12:11.719968Z","iopub.status.idle":"2021-11-27T11:12:22.807364Z","shell.execute_reply.started":"2021-11-27T11:12:11.719937Z","shell.execute_reply":"2021-11-27T11:12:22.806636Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# to play an audio\naudio1 = '/kaggle/input/voicetest/en-0527.wav'\nipd.Audio(audio1)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:12:22.808614Z","iopub.execute_input":"2021-11-27T11:12:22.809300Z","iopub.status.idle":"2021-11-27T11:12:22.864166Z","shell.execute_reply.started":"2021-11-27T11:12:22.809251Z","shell.execute_reply":"2021-11-27T11:12:22.863375Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# to convert the speech to text and classify sentiment\ntext_set=[]\nsenti_set=[]\nrecording = sr.Recognizer()\nfor filenameop in filelist:\n    with sr.AudioFile(filenameop) as source:\n        audio_data = recording.record(source)\n        text = recording.recognize_google(audio_data)\n        text = contractions.fix(text)\n        #print(text)\n        text_set.append(text)\n        tb = textblob.TextBlob(text, analyzer=textblob.sentiments.NaiveBayesAnalyzer())\n        senti = tb.sentiment \n        senti_set.append(senti)\n        print(\"\\nText:\", text) #, \"Classify:\",tb.classify())\n        print(senti)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:12:22.865967Z","iopub.execute_input":"2021-11-27T11:12:22.866241Z","iopub.status.idle":"2021-11-27T11:13:13.162335Z","shell.execute_reply.started":"2021-11-27T11:12:22.866206Z","shell.execute_reply":"2021-11-27T11:13:13.161431Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# to perform the topic analysis\ntexts=[]\nall_tokens=[]\nall_stop_tokens=[]\n\ntokenizer = RegexpTokenizer(r'\\w+')\nen_stop = get_stop_words('en')\np_stemmer = PorterStemmer()\nwnl = nltk.stem.wordnet.WordNetLemmatizer()\n\nfor i in text_set:\n    raw = i.lower()\n    tokens = tokenizer.tokenize(raw)\n    \n    stopped_tokens = [i for i in tokens if not i in en_stop]\n    lemma_tokens = [wnl.lemmatize(i) for i in stopped_tokens]\n    all_tokens.append(tokens)\n    all_stop_tokens.append(stopped_tokens)\n    texts.append(lemma_tokens)\n    \ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(txt) for txt in texts]\n\nprint('Tokens:',all_tokens,sep='\\n')\nprint('\\nStop_tokens:',all_stop_tokens,sep='\\n')\nprint('\\nLemma_tokens:',texts,sep='\\n')\nprint('\\nDictionary:',dictionary,sep='\\n')\nprint('\\nCorpus:',corpus,sep='\\n')\n\nlda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=20)\nprint('\\nTopics:',5,\"Words:\",5)\ndisplay(lda_model.print_topics(num_topics=5, num_words=5))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:13:13.163808Z","iopub.execute_input":"2021-11-27T11:13:13.164071Z","iopub.status.idle":"2021-11-27T11:13:14.801336Z","shell.execute_reply.started":"2021-11-27T11:13:13.164035Z","shell.execute_reply":"2021-11-27T11:13:14.800451Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# visualize the topic analysis\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary,mds='mmds')\npyLDAvis.display(vis)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:13:14.802993Z","iopub.execute_input":"2021-11-27T11:13:14.803325Z","iopub.status.idle":"2021-11-27T11:13:17.576121Z","shell.execute_reply.started":"2021-11-27T11:13:14.803268Z","shell.execute_reply":"2021-11-27T11:13:17.575070Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# to perform the entity analysis\nNER = spacy.load(\"en_core_web_sm\")\nfor t in text_set:\n    raw_txt = t #text_set[4]\n    ner_txt = NER(raw_txt)\n    print([(word, word.ent_type_) for word in ner_txt if word.ent_type_])","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:13:17.578010Z","iopub.execute_input":"2021-11-27T11:13:17.578386Z","iopub.status.idle":"2021-11-27T11:13:18.275173Z","shell.execute_reply.started":"2021-11-27T11:13:17.578336Z","shell.execute_reply":"2021-11-27T11:13:18.274255Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"named_entities = []\nfor sentence in text_set:\n    temp_entity_name = ''\n    temp_named_entity = None\n    sentence = NER(sentence)\n    for word in sentence:\n        term = word.text \n        tag = word.ent_type_\n        if tag:\n            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n            temp_named_entity = (temp_entity_name, tag)\n        #else:\n        if temp_named_entity:\n            named_entities.append(temp_named_entity)\n            temp_entity_name = ''\n            temp_named_entity = None\nentity_frame = pd.DataFrame(named_entities, \n                            columns=['Entity Name', 'Entity Type'])\ndisplay(entity_frame)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:13:18.276356Z","iopub.execute_input":"2021-11-27T11:13:18.276581Z","iopub.status.idle":"2021-11-27T11:13:18.348819Z","shell.execute_reply.started":"2021-11-27T11:13:18.276553Z","shell.execute_reply":"2021-11-27T11:13:18.347778Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# get the top named entities\ntop_entities = (entity_frame.groupby(by=['Entity Name', 'Entity Type'])\n                           .size()\n                           .sort_values(ascending=False)\n                           .reset_index().rename(columns={0 : 'Frequency'}))\ntop_entities.T.iloc[:,:15]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:13:18.351200Z","iopub.execute_input":"2021-11-27T11:13:18.351472Z","iopub.status.idle":"2021-11-27T11:13:18.369096Z","shell.execute_reply.started":"2021-11-27T11:13:18.351442Z","shell.execute_reply":"2021-11-27T11:13:18.368489Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# get the top named entity types\ntop_entities = (entity_frame.groupby(by=['Entity Type'])\n                           .size()\n                           .sort_values(ascending=False)\n                           .reset_index().rename(columns={0 : 'Frequency'}))\ntop_entities.T.iloc[:,:15]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T11:13:18.370794Z","iopub.execute_input":"2021-11-27T11:13:18.371503Z","iopub.status.idle":"2021-11-27T11:13:18.385753Z","shell.execute_reply.started":"2021-11-27T11:13:18.371464Z","shell.execute_reply":"2021-11-27T11:13:18.384873Z"},"trusted":true},"execution_count":12,"outputs":[]}]}